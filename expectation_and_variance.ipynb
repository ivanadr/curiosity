{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004e0e0-0dc0-48d2-ab2d-969c67656915",
   "metadata": {
    "id": "f004e0e0-0dc0-48d2-ab2d-969c67656915"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb61bce-a31e-4568-bb91-1fe9c5c4c728",
   "metadata": {
    "id": "fdb61bce-a31e-4568-bb91-1fe9c5c4c728"
   },
   "source": [
    "# Mean, Variance, Standard deviation, and Correlation coeffcient\n",
    "\n",
    "**Author**: Ivana Drabova\n",
    "\n",
    "**Objectives**: Deepen understanding of descriptive statistics through coding and comparing differnet approaches. Namely camcluaitng the mean and varaince with sums as well as expectations. Understanding biased or unbiased sample statistics and knowing which pyhton libaries use what as default (numpy vs pandas). Finally, seeing how the bias and non biased formulas affect covaraince and correaltion formulas so that I do not make mistakes later in my studies.\n",
    "\n",
    "**Questions** Sample vs population (roman vs greek letters). But is it okay to use the expectation formulas notation for samples? I think yes? Just if sample, call $E(X) = \\bar X$ instead of $E(X) = \\mu$.\n",
    "\n",
    "Same for var and corr ($\\sigma$ vs $s^2$; $\\rho$ vs $r_{xy}$)? But using E notation and the words Cov(X,Y) and Var(X,Y) should be okay in all cases?\n",
    "\n",
    "${\\textstyle \\left({\\frac {x_{i}-{\\bar {x}}}{s_{x}}}\\right)}$ is standard score and can be used in one of the correlation formulas. Cool, I forgot the name of this. Just tended to reffered to it as 'standardization'. Would this be the same as z score? I think so but rather check.  Why is it called z? hmm, I forget it has been a while since I used the actual table.  \n",
    "\n",
    "I am not sure of the explanation behind bias. I also wonder when we unbias we increase the $s^2$ (compared to the biased one), does this raise threashold for significane (i.e. it is harder for results to be significant? I think yes, intuitively. But should check. this wuld also explain the 'conservativeness of scientists' (paraphrased) used by Neil Salkind in the basic book.\n",
    "\n",
    "* Minor note, the n in the picture should be capitalized to N such that the following discussion makes perfect sense.  is this true ??\n",
    "\n",
    "**Motivation**   \n",
    "\n",
    "1. artihmetic mean and expectation are computed as $\\bar {X} = \\sum{\\frac{X}{n}}$ and ${E} [X] = \\sum{x P(X=x)}$.   \n",
    "How can I do the latter explicitely in python pandas and numpy? (disregard implementing ${\\displaystyle {E} [X]=\\int _{-\\infty }^{+\\infty }x f(x)\\,\\mathrm {d} x}$ for now)\n",
    "\n",
    "\n",
    "2. In the spirit of using expectation in python from (1) I will also calculate Variance from the variance with the expectaion formula. I care about implementation here and so will again asume the sample is the population $n=N$\n",
    "$$\n",
    "{\\displaystyle {\\begin{aligned}\\sigma _{X}^{2}={}&\\operatorname {\\mathbb {E} } \\left[\\,\\left(X-\\operatorname {\\mathbb {E} } [X]\\right)^{2}\\,\\right]=\\operatorname {\\mathbb {E} } \\left[\\,X^{2}\\,\\right]-\\left(\\operatorname {\\mathbb {E} } [\\,X\\,]\\right)^{2}\\end{aligned}}}\n",
    "$$\n",
    "\n",
    "$$Var [X] = \\sum{ P(X=x) (x - \\bar x )}$$\n",
    "\n",
    "As average of sum of squared deviations\n",
    "$${\\textstyle s^2_{x,biased}={ {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$$\n",
    "\n",
    "\n",
    "\n",
    "3. the standard deviation formulas differ by divisin by $n$ (biased) as opposed to $n-1$ (unbiased). Is python (pandas and numpy) using unbiased or biased variance?\n",
    "$${\\textstyle s_{x,unbiased}={\\sqrt {{\\frac {1}{n-1}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$$\n",
    "\n",
    "$${\\textstyle s_{x,biased}={\\sqrt {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$$\n",
    "\n",
    "\n",
    "4. Preparing for correlation, I will code the covaraince formula:\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}\\\\&\\operatorname {\\mathbb {E} } [\\,\\left(X-\\mu _{X}\\right)\\left(Y-\\mu _{Y}\\right)\\,]=\\operatorname {\\mathbb {E} } [\\,\\left(X-\\operatorname {\\mathbb {E} } [\\,X\\,]\\right)\\left(Y-\\operatorname {\\mathbb {E} } [\\,Y\\,]\\right)\\,]=\\operatorname {\\mathbb {E} } [\\,X\\,Y\\,]-\\operatorname {\\mathbb {E} } [\\,X\\,]\\operatorname {\\mathbb {E} } [\\,Y\\,]\\,,\\end{aligned}}}$$\n",
    "\n",
    "But since we I will ahve had enough practice of the expectations at this point, I will just use the version with sums.  $${\\displaystyle \\operatorname {cov} (X,Y)={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-E(X))(y_{i}-E(Y)).}$$\n",
    "\n",
    "5. Finaly, correlation coefficient is somewhat tricky, becasue there are multiple formulas for it. Can I code and check the most common formuals as well as the alternative formulas for $r_{xy}$ and in the process improve my understanding of $r_{xy}$? I am especially interested in errors that can happen when combining biased and unbiased sample statistics, hence combingin knowledge from point (2).\n",
    "\n",
    "For populaiton (as reference, no implementation):    \n",
    "$${\\displaystyle \\rho _{X,Y}={\\frac {\\operatorname {cov} (X,Y)}{\\sigma _{X}\\sigma _{Y}}}}$$\n",
    "\n",
    "$${\\displaystyle \\rho _{X,Y}={\\frac {\\operatorname {\\mathbb {E} } [(X-\\mu _{X})(Y-\\mu _{Y})]}{\\sigma _{X}\\sigma _{Y}}}}$$  \n",
    "\n",
    "For sample (derive and implement with pandas):   \n",
    "$$ {\\displaystyle r_{xy}={\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}} $$\n",
    "\n",
    "$${\\displaystyle r_{xy}={\\frac {n\\sum x_{i}y_{i}-\\sum x_{i}\\sum y_{i}}{{\\sqrt {n\\sum x_{i}^{2}-\\left(\\sum x_{i}\\right)^{2}}}~{\\sqrt {n\\sum y_{i}^{2}-\\left(\\sum y_{i}\\right)^{2}}}}}.}$$\n",
    "\n",
    "\n",
    "$${\\displaystyle r_{xy}={\\frac {\\sum _{i}x_{i}y_{i}-n{\\bar {x}}{\\bar {y}}}{{\\sqrt {\\sum _{i}x_{i}^{2}-n{\\bar {x}}^{2}}}~{\\sqrt {\\sum _{i}y_{i}^{2}-n{\\bar {y}}^{2}}}}}.}$$\n",
    "\n",
    "$${\\displaystyle r_{xy}={\\frac {1}{n-1}}\\sum _{i=1}^{n}\\left({\\frac {x_{i}-{\\bar {x}}}{s_{x}}}\\right)\\left({\\frac {y_{i}-{\\bar {y}}}{s_{y}}}\\right)}$$\n",
    "\n",
    "$${\\displaystyle r_{xy}={\\frac {\\sum x_{i}y_{i}-n{\\bar {x}}{\\bar {y}}}{(n-1)s_{x}s_{y}}}}$$\n",
    "\n",
    "For linear regression: $Y_i = \\alpha + \\beta X_i + \\varepsilon_i$\n",
    "* $$ \\hat {\\beta} = {\\rm cor}(y, x) \\cdot \\frac{ {\\rm s}_y }{ {\\rm s}_x } $$\n",
    "* $$ \\hat {\\beta} = {\\displaystyle {\\frac {\\sum x_{i}y_{i}-n{\\bar {x}}{\\bar {y}}}{(n-1)s_{x}s_{y}}}} \\cdot \\frac{ {\\rm s}_y }{ {\\rm s}_x } $$\n",
    "See $s_y$ cancel out, while $s_x$ is squared\n",
    "* $$ \\hat {\\beta} = \\frac {cov(x,y)}{var(x)}$$\n",
    "\n",
    "**Take aways:**  \n",
    "\n",
    "\n",
    "**Further steps**\n",
    "integration - calculate expectation with integral ${\\displaystyle {E} [X]=\\int _{-\\infty }^{+\\infty }x f(x)\\,\\mathrm {d} x}$\n",
    "https://pythonnumericalmethods.berkeley.edu/notebooks/chapter20.01-Numerical-Differentiation-Problem-Statement.html\n",
    "\n",
    "5. ${\\textstyle \\left({\\frac {x_{i}-{\\bar {x}}}{s_{x}}}\\right)}$ is standard score and can be used in one of the correlation formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f2b27-fdc1-4db9-a9bc-c0bc596c97ec",
   "metadata": {
    "id": "526f2b27-fdc1-4db9-a9bc-c0bc596c97ec"
   },
   "source": [
    "**Useful links**\n",
    "https://stats.stackexchange.com/questions/3931/intuitive-explanation-for-dividing-by-n-1-when-calculating-standard-deviation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Variance\n",
    "\n",
    "https://en.m.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "\n",
    "\n",
    "**Data** from Neil Salkind's book *Statistics for people who think they hate statistics*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369ab9f-ffdb-47b2-ae21-6916ac5702ef",
   "metadata": {
    "id": "4369ab9f-ffdb-47b2-ae21-6916ac5702ef",
    "tags": []
   },
   "source": [
    "# Bias in Standard deviation and Variance refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6a914-fb07-4026-8fb6-e17e241725c2",
   "metadata": {
    "id": "53f6a914-fb07-4026-8fb6-e17e241725c2"
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Variance_visualisation.svg/220px-Variance_visualisation.svg.png)  \n",
    "\"Geometric visualisation of the variance of an arbitrary distribution (2, 4, 4, 4, 5, 5, 7, 9):\n",
    "- A frequency distribution is constructed.\n",
    "- The centroid of the distribution gives its mean.\n",
    "- A square with sides equal to the difference of each value from the mean is formed for each value.\n",
    "- Arranging the squares into a rectangle with one side equal to the number of values, n , results in the other side being the distribution's variance, Ïƒ2.\"\n",
    "Source: https://en.wikipedia.org/wiki/Variance  \n",
    "* Minor note, the n in the picture should be capitalized to N such that the following discussion makes perfect sense.\n",
    "\n",
    "**The picture above**   \n",
    "- the rectangle has area $\\sigma^2 * N$\n",
    "- $\\sigma^2 * N$ equals sum of the squared devations from the mean $ \\sum{}{(x-{\\bar {x}})}^2$ i.e **almost** the definition of variance. We still need to divide both sides by n to get the variace equation right:\n",
    "\n",
    "1.  $\\sigma^2 * N =\\sigma^2 * N$ ; move N to the other side\n",
    "2. Variance: $\\sigma^2 = \\frac{\\sum{}{(x-{\\bar {x}})}^2}{N}$\n",
    "\n",
    "Intuition behind 1. and 2.:  \n",
    "- In the picture above we imagine dividing the square into $N$ parts\n",
    "    - a realtively thin rectangle with height $\\sigma^2$\n",
    "    - Sidenote: when N large it becomes very thin rectangle just like with limits and then the division to n or n-1 parts will not really make a differnece (ie unbiased measure approaches the biased measure in limit).\n",
    "- $\\sigma^2$ is our variance.\n",
    "\n",
    "Problems:\n",
    "- $\\sigma^2$ is expressed in squared units\n",
    "    - I do not know how to think in squares (e.g. kilograms-squared)\n",
    "    - I want to take square root to get to original units\n",
    "    - I want to be able to say, for example, someting about expected spread of kilograms from the mean kilogram value of my population (i.e do not want kilograms squared but just kilograms).\n",
    "- hence, we take square root of the line with height $\\sigma^2$ to get what we call standard deviation $\\sigma$.\n",
    "\n",
    "**Standard deviation $\\sigma$** is square root of variance $\\sigma^2$\n",
    "\n",
    "If we deal with samples we rename $\\sigma^2$ to $s^2$ and $\\sigma$ to $s$ and slightly modify the formula. Why?\n",
    "\n",
    "Since I do not have data for all people in the world $(N)$, I have to be satisifed with the few $(n)$ individuals in my sample. If we divide the sum of squares $ \\sum{}{(x-{\\bar {x}})}^2$ by $n$ there is a problem of downward bias (we underestimate variance) [see discussion here](https://stats.stackexchange.com/questions/3931/intuitive-explanation-for-dividing-by-n-1-when-calculating-standard-deviation).   \n",
    "TLDR, if we had the population, dividing $ \\sum{}{(x-{\\bar {x}})}^2$ by $n$ is fine becasue $n = N$.\n",
    "\n",
    "But becasue we only have a sample, we need to divide $ \\sum{}{(x-{\\bar {x}})}^2$ by $n-1$.\n",
    "\n",
    "Hence, arriving at the following two formulas for biased and unbiased standard deviation for a sample:\n",
    "$${\\displaystyle s_{biased}={\\sqrt {{\\frac {1}{n}}\\sum _{i=1}^{n}\\left(x_{i}-{\\bar {x}}\\right)^{2}}},}$$\n",
    "\n",
    "\n",
    "$${\\displaystyle s_{unbiased}={\\sqrt {{\\frac {1}{n-1}}\\sum _{i=1}^{n}\\left(x_{i}-{\\bar {x}}\\right)^{2}}}} = s$$\n",
    "\n",
    "Which one does python use in Numpy and Pandas libraries? (Find out in Task 2,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9966e-beb1-42e5-9af3-66cffe83f6fb",
   "metadata": {
    "id": "73b9966e-beb1-42e5-9af3-66cffe83f6fb"
   },
   "source": [
    "Sidenote before moving on: ilustation of using $n-1$\n",
    "\n",
    "![My Image](rectangle.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671ef1d-46a0-4fb9-858c-738d40a7ff3d",
   "metadata": {
    "id": "7671ef1d-46a0-4fb9-858c-738d40a7ff3d"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231fbf03-cde1-44a1-9a0c-30da92d55ef3",
   "metadata": {
    "id": "231fbf03-cde1-44a1-9a0c-30da92d55ef3"
   },
   "source": [
    "- Artihmetic mean (average): $$\\bar {X} = \\sum{\\frac{X}{n}}$$  \n",
    "- Is the same as expectation for discrete random varaible: $${E} [X] = \\sum{x P(X=x) }$$\n",
    "    - (value of x times probability of that value occuring)\n",
    "\n",
    "- Likewise, for continuous random varaible with probability mass funciton $f$: $${\\displaystyle {E} [X]=\\int _{-\\infty }^{+\\infty }x f(x)\\,\\mathrm {d} x}$$\n",
    "\n",
    "- All of the above are equivalent when dealing with a sample in our example.\n",
    "    - Why? Becasue we assume that $P(X=x) = \\frac{1}{n}$ for all $x$.\n",
    "    - Although we have a discrete version so we really are working with the first two formulas.\n",
    "    - Disregrad implementing ${\\displaystyle {E} [X]=\\int _{-\\infty }^{+\\infty }x f(x)\\,\\mathrm {d} x}$ for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0457e2e-d94d-4e80-a923-831edd09e23f",
   "metadata": {
    "id": "f0457e2e-d94d-4e80-a923-831edd09e23f"
   },
   "outputs": [],
   "source": [
    "df = pd.read_spss('hate/DataSets/Chapter 5 Data Set 2.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e45c1-a370-4079-8adf-043cc0d6a1b1",
   "metadata": {
    "id": "0c9e45c1-a370-4079-8adf-043cc0d6a1b1",
    "outputId": "7f7da70b-2ff2-4b6a-a3e5-f7825f8d0523"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x     y\n",
       "0  17.0  94.0\n",
       "1  13.0  73.0\n",
       "2  12.0  59.0\n",
       "3  15.0  80.0\n",
       "4  16.0  93.0\n",
       "5  14.0  85.0\n",
       "6  16.0  66.0\n",
       "7  16.0  79.0\n",
       "8  18.0  77.0\n",
       "9  19.0  91.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['x', 'y']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f253b-e809-412f-a0e0-16028afdeaf9",
   "metadata": {
    "id": "444f253b-e809-412f-a0e0-16028afdeaf9",
    "outputId": "5b5ea381-eb27-4ac0-deb8-517e07ba681c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.600000</td>\n",
       "      <td>79.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.170509</td>\n",
       "      <td>11.576317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.250000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>79.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.750000</td>\n",
       "      <td>89.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x          y\n",
       "count  10.000000  10.000000\n",
       "mean   15.600000  79.700000\n",
       "std     2.170509  11.576317\n",
       "min    12.000000  59.000000\n",
       "25%    14.250000  74.000000\n",
       "50%    16.000000  79.500000\n",
       "75%    16.750000  89.500000\n",
       "max    19.000000  94.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c8f3e-938c-4379-acb8-527c617c8b1a",
   "metadata": {
    "id": "0c9c8f3e-938c-4379-acb8-527c617c8b1a"
   },
   "source": [
    "- Artihmetic mean (average): $$\\bar {X} = \\sum{\\frac{X}{n}}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b494ce3-9c3b-4de4-99aa-a43c7f9c4bd8",
   "metadata": {
    "id": "8b494ce3-9c3b-4de4-99aa-a43c7f9c4bd8",
    "outputId": "655b4b62-fd89-4ee1-e736-6ff9644da64e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee000b-9f78-4f10-b319-e2111391a367",
   "metadata": {
    "id": "b9ee000b-9f78-4f10-b319-e2111391a367",
    "outputId": "cca11ae6-8681-495e-bcd1-d19efc53bc99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.x.sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc04dc4-00ac-4d68-993f-d7f7fa54209f",
   "metadata": {
    "id": "ffc04dc4-00ac-4d68-993f-d7f7fa54209f"
   },
   "source": [
    "Practice expectations implementation\n",
    "$${E} [X] = \\sum{x P(X=x) }$$\n",
    "    - (value of x times probability of that value occuring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ea7e0-4df0-45d0-8f5a-d3367b0455c3",
   "metadata": {
    "id": "c55ea7e0-4df0-45d0-8f5a-d3367b0455c3",
    "outputId": "b3c81920-7d0b-43d7-e5ec-01dfaa2f3dad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0    0.3\n",
       "17.0    0.1\n",
       "13.0    0.1\n",
       "12.0    0.1\n",
       "15.0    0.1\n",
       "14.0    0.1\n",
       "18.0    0.1\n",
       "19.0    0.1\n",
       "Name: (x, px), dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px = df['x'].value_counts(normalize=True)\n",
    "px.rename(index='(x, px)')\n",
    "# probability mass funciton px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7373c69-88d3-45bf-8ebf-43e6793057f0",
   "metadata": {
    "id": "e7373c69-88d3-45bf-8ebf-43e6793057f0",
    "outputId": "fe6730fe-9d16-429b-ca4e-04b87edff5d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.sum() #values should equl to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a59ca5-8e24-4134-ab7f-46626da7a0f6",
   "metadata": {
    "id": "e6a59ca5-8e24-4134-ab7f-46626da7a0f6",
    "outputId": "f533ccab-3d0e-4375-9352-c32f5834036a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc50426-811b-426a-bc0e-64ca460f4b4a",
   "metadata": {
    "id": "2fc50426-811b-426a-bc0e-64ca460f4b4a"
   },
   "source": [
    "$${E} [X] = \\sum{x P(X=x) }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40eb49-a6ad-4c48-b4f1-efe58be1d7b8",
   "metadata": {
    "id": "2b40eb49-a6ad-4c48-b4f1-efe58be1d7b8",
    "outputId": "4e85803e-8a8e-4a0f-d5ea-7f0ced840c90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.600000000000001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean as expectation: meaning sum of (x * p(x)\n",
    "x_bar = pd.Series(px.index*px.values).sum()\n",
    "x_bar #similar to the one in describe (difference in last bit - this is unimportant and relted to computer memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c998609-2354-43ec-912d-4c494fccb0c4",
   "metadata": {
    "id": "7c998609-2354-43ec-912d-4c494fccb0c4"
   },
   "source": [
    "Analogous with y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede6f0e-dd06-4c41-88ee-9ff1a9338af0",
   "metadata": {
    "id": "dede6f0e-dd06-4c41-88ee-9ff1a9338af0",
    "outputId": "702bc66a-1228-4c32-9f18-0b41e6e305b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.0    0.1\n",
      "73.0    0.1\n",
      "59.0    0.1\n",
      "80.0    0.1\n",
      "93.0    0.1\n",
      "85.0    0.1\n",
      "66.0    0.1\n",
      "79.0    0.1\n",
      "77.0    0.1\n",
      "91.0    0.1\n",
      "Name: y, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same with y\n",
    "#probability mass funcitonypz\n",
    "py = df['y'].value_counts(normalize=True)\n",
    "print(py)\n",
    "py.values.sum() #here we also check if sum of probability mass funciton equals one as it should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d0921-787f-45bc-9948-bce24c8a1ef2",
   "metadata": {
    "id": "338d0921-787f-45bc-9948-bce24c8a1ef2",
    "outputId": "78d6f7a3-ab53-4694-b414-cab17dcaab3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.69999999999999\n",
      "79.7\n"
     ]
    }
   ],
   "source": [
    "y_bar = pd.Series(py.index*py.values).sum() #mean y calcualted as expectation\n",
    "print(y_bar) #indeed it is a bit of an overkill it would be easier to do the classic sum/#n items as below\n",
    "#and we can see that becasue we are not summing so much we also have a bit more precision with bits accumulating\n",
    "print(df.y.sum()/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b86ef-9d90-494e-bdd9-511b574d56ed",
   "metadata": {
    "id": "c43b86ef-9d90-494e-bdd9-511b574d56ed"
   },
   "outputs": [],
   "source": [
    "x_bar = df.x.mean()\n",
    "y_bar = df.y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a771ef67-8076-4233-ab7d-a3fdc125248e",
   "metadata": {
    "id": "a771ef67-8076-4233-ab7d-a3fdc125248e"
   },
   "source": [
    "# Takeaway 1:\n",
    "* Implemented both methods of calcuating arith. mean: $\\bar {X} = \\sum{\\frac{X}{n}}$ and   ${E} [X] = \\sum{x P(X=x) }$\n",
    "- did not implement ${\\displaystyle {E} [X]=\\int _{-\\infty }^{+\\infty }x f(x)\\,\\mathrm {d} x}$. Leaving it for furture.\n",
    "* important parameter to get probability mass function P(X) in `value counts parameter normalize `\n",
    "* Pandas and numpy are using the $\\bar {X} = \\sum{\\frac{X}{n}}$ approach as opposed to ${E} [X] = \\sum{x P(X=x) }$\n",
    "* ${E} [X] = \\sum{x P(X=x) }$ is an overkill and leads to tiny error with bits accumulation (but this difference is very small)\n",
    "    * with multiplication we have to align decimal point in the floating system - this is quite a niche remark from computer systems :) and it is good to know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3b89b-cd2c-4711-b780-014f2e9da08f",
   "metadata": {
    "id": "52e3b89b-cd2c-4711-b780-014f2e9da08f"
   },
   "source": [
    "# Task 2:\n",
    "2. In the spirit of using expectation in python from (1) I will also calculate Variance from the variance with the expectaion formula. I care about implementation here and so will again asume the sample is the population $n=N$\n",
    "$$\n",
    "{\\displaystyle {\\begin{aligned}\\sigma _{X}^{2}={}&\\operatorname {\\mathbb {E} } \\left[\\,\\left(X-\\operatorname {\\mathbb {E} } [X]\\right)^{2}\\,\\right]=\\operatorname {\\mathbb {E} } \\left[\\,X^{2}\\,\\right]-\\left(\\operatorname {\\mathbb {E} } [\\,X\\,]\\right)^{2}\\end{aligned}}}\n",
    "$$\n",
    "\n",
    "$$Var [X] = \\sum{ P(X=x) (x - \\bar x )}$$\n",
    "\n",
    "As average of sum of squared deviations\n",
    "$${\\textstyle s^2_{x,biased}={ {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f2054-2539-4e69-895d-4c6be179021e",
   "metadata": {
    "id": "361f2054-2539-4e69-895d-4c6be179021e",
    "outputId": "e8b0e5cc-e852-4974-ce3b-dbb6fd0548e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as expectations 1\n",
      "4.239999999999952\n",
      "as expectations 1b\n",
      "4.240000000000009\n",
      "expectations 2:\n",
      "4.24\n",
      "as average sum of squared deviations (divide by n)\n",
      "4.24\n"
     ]
    }
   ],
   "source": [
    "#standard deviation of x calulated with expectations\n",
    "\n",
    "# px.values = probability\n",
    "# px.index = x\n",
    "\n",
    "print('as expectations 1')\n",
    "var_x = sum(px.values*px.index**2) - (sum(px.values*px.index))**2\n",
    "print(var_x)\n",
    "print('as expectations 1b')\n",
    "var_x = sum(px.values*px.index**2) - (x_bar)**2\n",
    "print(var_x)\n",
    "\n",
    "print('expectations 2:')\n",
    "var_x = sum(px.values*((px.index-x_bar)**2))\n",
    "print(var_x)\n",
    "\n",
    "print('as average sum of squared deviations (divide by n)')\n",
    "df['x-xbar'] = df[\"x\"]-df[\"x\"].mean()\n",
    "print((df['x-xbar']**2).sum()/(df.shape[0])) #biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e22b15-97df-49c7-a9c2-0fe3afc0bb7e",
   "metadata": {
    "id": "e0e22b15-97df-49c7-a9c2-0fe3afc0bb7e",
    "outputId": "a28b21e4-054e-4894-b844-a5eea7d45c61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df.x) #numpy default is divide b n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee7410-cdef-40c2-b1d9-eb97beabcecf",
   "metadata": {
    "id": "b3ee7410-cdef-40c2-b1d9-eb97beabcecf",
    "outputId": "887fe864-2c2e-4fce-e06d-f486d2270fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.x.var(ddof=0) #pandas default is divide by n-1, hence I had to adjust ddof parameter to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e50607-c0a7-4c2a-9f87-1152adad456f",
   "metadata": {
    "id": "51e50607-c0a7-4c2a-9f87-1152adad456f"
   },
   "source": [
    "# Takeaway 2:\n",
    "* sucessfully implemented calculation of VAR of three expectaiton methods using probabiliy functions $P(X=x$: $\n",
    "{\\displaystyle {\\begin{aligned}\\sigma _{X}^{2}={}&\\operatorname {\\mathbb {E} } \\left[\\,\\left(X-\\operatorname {\\mathbb {E} } [X]\\right)^{2}\\,\\right], \\end{aligned}} second {\\begin{aligned}\\operatorname {\\mathbb {E} } \\left[\\,X^{2}\\,\\right]-\\left(\\operatorname {\\mathbb {E} } [\\,X\\,]\\right)^{2}\\end{aligned}}}$ and third $Var [X] = \\sum{ P(X=x) (x - \\bar x )}$\n",
    "* successfully impolemented calcualtion of sd with average of sum of squares method ${\\textstyle s^2_{x,biased}={ {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$\n",
    "\n",
    "\n",
    "- assuming $n=N$, our manual calcualtion is eqivalent in calculation to biased sample varaince $s^2_{biased}$\n",
    "\n",
    "$$\n",
    "{\\displaystyle {\\begin{aligned}\\sigma _{X}^{2}={}&\\operatorname {\\mathbb {E} } \\left[\\,\\left(X-\\operatorname {\\mathbb {E} } [X]\\right)^{2}\\,\\right]=\\operatorname {\\mathbb {E} } \\left[\\,X^{2}\\,\\right]-\\left(\\operatorname {\\mathbb {E} } [\\,X\\,]\\right)^{2}\\end{aligned}}}  = \\text{in our example with uniform probability as if we did} = {\\textstyle s^2_{x,biased}={ {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}\n",
    "$$\n",
    "\n",
    "* but these two approaches are not conceptually the same since one is sample and the otehr is populaiton\n",
    "* and numerically, the are also not the same as there are rounding errors - the sum of squared diviation calculation is better just like in Task (1) due to rounding, and $Var [X] = \\sum{ P(X=x) (x - \\bar x )}$ is the method that pandas and numpy are usuing (as oposed to expectation with px)\n",
    "\n",
    "**yet there are differences in pandas and numpy's default settings\n",
    "* Pandas's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74087380-7695-4448-8159-4fd0d74ea4ec",
   "metadata": {
    "id": "74087380-7695-4448-8159-4fd0d74ea4ec"
   },
   "source": [
    "# Taks 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19125c4d-0809-4ebc-b8dd-0f44c030662f",
   "metadata": {
    "id": "19125c4d-0809-4ebc-b8dd-0f44c030662f"
   },
   "source": [
    "Pandas default is that the dataframe is sample. Hence, default standard devaition in both `describe` and `std` commands is adjusted to that of a sample. Hence the variance will be unbiased too. As we see below the variance is higher than that calculated in Task 2.\n",
    "\n",
    "Task (2) numerically it is the same to do  $${\\textstyle s_{x,biased}={\\sqrt {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$$\n",
    "\n",
    "Pandas default\n",
    "$${\\textstyle s_{x,unbiased}={\\sqrt {{\\frac {1}{n-1}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$$\n",
    "\n",
    "Numpy default\n",
    "$${\\textstyle s_{x,biased}={\\sqrt {{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51fd6a3-0361-4ed5-a904-c379949dc74e",
   "metadata": {
    "id": "d51fd6a3-0361-4ed5-a904-c379949dc74e",
    "outputId": "903ea2ac-a527-4992-9d00-c935e7fb348d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default Pandas\n",
      "pandas std x 2.1705094128132942\n",
      "pandas std y 11.576316819745005\n"
     ]
    }
   ],
   "source": [
    "print('default Pandas')\n",
    "print('pandas std x', df.x.std())\n",
    "print('pandas std y', df.y.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbedb7-e29f-4a2c-a013-e81ef31b1ac0",
   "metadata": {
    "id": "3fdbedb7-e29f-4a2c-a013-e81ef31b1ac0",
    "outputId": "b0a8c543-36e0-478f-8c86-0ae202331b14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x-xbar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.600000</td>\n",
       "      <td>79.700000</td>\n",
       "      <td>3.552714e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.170509</td>\n",
       "      <td>11.576317</td>\n",
       "      <td>2.170509e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>-3.600000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.250000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>-1.350000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>4.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.750000</td>\n",
       "      <td>89.500000</td>\n",
       "      <td>1.150000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>3.400000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x          y        x-xbar\n",
       "count  10.000000  10.000000  1.000000e+01\n",
       "mean   15.600000  79.700000  3.552714e-16\n",
       "std     2.170509  11.576317  2.170509e+00\n",
       "min    12.000000  59.000000 -3.600000e+00\n",
       "25%    14.250000  74.000000 -1.350000e+00\n",
       "50%    16.000000  79.500000  4.000000e-01\n",
       "75%    16.750000  89.500000  1.150000e+00\n",
       "max    19.000000  94.000000  3.400000e+00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2accc0-07cd-4075-aba8-555afe351c60",
   "metadata": {
    "id": "9c2accc0-07cd-4075-aba8-555afe351c60"
   },
   "source": [
    "I did not find a way aorund this for the `describe method` but the `std` and `var` commands have the `ddof=1` parameter which means default is $n-1$ division of the sum of square deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f759e-5e0e-4ede-addc-4df65ba7cffc",
   "metadata": {
    "id": "d39f759e-5e0e-4ede-addc-4df65ba7cffc",
    "outputId": "ab5c7798-87d2-4091-9835-852b2cd9d460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdx, sd y \n",
      "default Pandas: n-1 : unbiased\n",
      "2.1705094128132942 11.576316819745005\n",
      "ajd parameter ddof: when wanting n, e.g. when dealing with populaitons\n",
      "2.0591260281974 10.982258419833325\n"
     ]
    }
   ],
   "source": [
    "print('sdx, sd y ')\n",
    "print('default Pandas: n-1 : unbiased')\n",
    "print(np.sqrt(df.x.var()), np.sqrt(df.y.var()))\n",
    "print('ajd parameter ddof: when wanting n, e.g. when dealing with populaitons')\n",
    "print(np.sqrt(df.x.var(ddof=0)), np.sqrt(df.y.var(ddof=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c64d1-dcbf-4f8e-9a55-f1dc7fb3bc48",
   "metadata": {
    "id": "5c3c64d1-dcbf-4f8e-9a55-f1dc7fb3bc48",
    "outputId": "208a808c-b2c6-40f1-85e1-e409a67d4b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdx, sd y \n",
      "default Numpy: n : biased\n",
      "2.0591260281974 10.982258419833325\n",
      "ajd parameter ddo to 1 to unbias Numpy\n",
      "2.1705094128132942 11.576316819745005\n"
     ]
    }
   ],
   "source": [
    "print('sdx, sd y ')\n",
    "print('default Numpy: n : biased')\n",
    "print(np.sqrt(np.var(df.x)), np.sqrt(np.var(df.y)))\n",
    "print('ajd parameter ddo to 1 to unbias Numpy')\n",
    "print(np.sqrt(df.x.var(ddof=1)), np.sqrt(df.y.var(ddof=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8714697-8e7a-4a37-8d10-99185e550584",
   "metadata": {
    "id": "b8714697-8e7a-4a37-8d10-99185e550584"
   },
   "source": [
    "# Takeaway 2:\n",
    "We conclude that pandas used unbiased standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125a189-8223-483f-ba95-382a42a8db38",
   "metadata": {
    "id": "5125a189-8223-483f-ba95-382a42a8db38"
   },
   "outputs": [],
   "source": [
    "#we can chekc whether that is the case too, by calculating NOT with expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa6cbd-2eba-48a1-9eb2-13627783e41b",
   "metadata": {
    "id": "71aa6cbd-2eba-48a1-9eb2-13627783e41b",
    "outputId": "4b383a54-56b0-4a6f-e3e7-dab54f755771"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x-xbar</th>\n",
       "      <th>y-ybar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>-20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x     y  x-xbar  y-ybar\n",
       "0  17.0  94.0     1.4    14.3\n",
       "1  13.0  73.0    -2.6    -6.7\n",
       "2  12.0  59.0    -3.6   -20.7\n",
       "3  15.0  80.0    -0.6     0.3\n",
       "4  16.0  93.0     0.4    13.3\n",
       "5  14.0  85.0    -1.6     5.3\n",
       "6  16.0  66.0     0.4   -13.7\n",
       "7  16.0  79.0     0.4    -0.7\n",
       "8  18.0  77.0     2.4    -2.7\n",
       "9  19.0  91.0     3.4    11.3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['x-xbar'] = df[\"x\"]-df[\"x\"].mean()\n",
    "df['y-ybar'] = df[\"y\"]-df[\"y\"].mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f6135-d9a4-44bc-a217-c574bf366411",
   "metadata": {
    "id": "d65f6135-d9a4-44bc-a217-c574bf366411",
    "outputId": "b7c257a5-e6b4-406b-9665-fce208fc71f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased sd 2.0591260281974\n",
      "unbiased sd (i.e n minus one in denom) 2.1705094128132942\n",
      "biased var 4.24\n",
      "unbiased var (i.e n minus one in denom) 4.711111111111111\n"
     ]
    }
   ],
   "source": [
    "print('biased sd',np.sqrt((df['x-xbar']**2).sum()/df.shape[0])) #biased - what we got with expectation\n",
    "print('unbiased sd (i.e n minus one in denom)', np.sqrt((df['x-xbar']**2).sum()/(df.shape[0]-1))) #unbiased\n",
    "\n",
    "print('biased var',(df['x-xbar']**2).sum()/df.shape[0]) #biased - what we got with expectation\n",
    "print('unbiased var (i.e n minus one in denom)', (df['x-xbar']**2).sum()/(df.shape[0]-1)) #unbiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e2de7-fbe1-48ac-a5a2-b23ea67cec3c",
   "metadata": {
    "id": "d40e2de7-fbe1-48ac-a5a2-b23ea67cec3c",
    "outputId": "a0cc02f7-f12f-45bd-cbb8-ab45eafcb6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased sd 10.982258419833325\n",
      "unbiased sd (i.e n minus one in denom) 11.576316819745005\n",
      "biased var 120.60999999999999\n",
      "unbiased var (i.e n minus one in denom) 134.0111111111111\n"
     ]
    }
   ],
   "source": [
    "print('biased sd',np.sqrt((df['y-ybar']**2).sum()/df.shape[0])) #biased - what we got with eypectation\n",
    "print('unbiased sd (i.e n minus one in denom)', np.sqrt((df['y-ybar']**2).sum()/(df.shape[0]-1))) #unbiased\n",
    "\n",
    "print('biased var',(df['y-ybar']**2).sum()/df.shape[0]) #biased - what we got with eypectation\n",
    "print('unbiased var (i.e n minus one in denom)', (df['y-ybar']**2).sum()/(df.shape[0]-1)) #unbiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6979d4a-3e76-47fc-8d49-531216e38138",
   "metadata": {
    "id": "e6979d4a-3e76-47fc-8d49-531216e38138",
    "outputId": "cb019bc7-480f-4d99-fed0-1d97160830ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x     2.170509\n",
       "y    11.576317\n",
       "Name: std, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().iloc[2][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd41c1-966a-4693-87d9-90b9012ab966",
   "metadata": {
    "id": "51fd41c1-966a-4693-87d9-90b9012ab966"
   },
   "outputs": [],
   "source": [
    "#indeed the describe comand uses unbiased one\n",
    "#with large sample size we expect that the biased one converges to biased one (the difference between them wil be small)\n",
    "# still it is useful to know whicih one we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef913958-f9ff-4bbf-bc7d-96d334558196",
   "metadata": {
    "id": "ef913958-f9ff-4bbf-bc7d-96d334558196",
    "outputId": "f4ccde59-2d80-4872-affb-efe6dbb231a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also check common learning that sum of deviations from mean is zero\n",
    "df['x-xbar'].sum() #need to round becasue of rounding error in bits floting point system\n",
    "np.round(df['x-xbar'].sum(), 10) #round to 10 digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273c316-9cf1-485f-bf33-bd2b10bbe438",
   "metadata": {
    "id": "d273c316-9cf1-485f-bf33-bd2b10bbe438",
    "outputId": "5736663e-5e85-4121-fbbf-e4742bd8eff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(df['y-ybar'].sum(), 10) #round to 10 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71986393-acfe-4fc1-89fc-bdc8d44fcb80",
   "metadata": {
    "id": "71986393-acfe-4fc1-89fc-bdc8d44fcb80"
   },
   "source": [
    "# covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec2161-d197-454c-9a6d-7ec14e4aa602",
   "metadata": {
    "id": "d3ec2161-d197-454c-9a6d-7ec14e4aa602",
    "outputId": "8533f571-9a5d-4534-e444-d9676733c243"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x-xbar</th>\n",
       "      <th>y-ybar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>-20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x     y  x-xbar  y-ybar\n",
       "0  17.0  94.0     1.4    14.3\n",
       "1  13.0  73.0    -2.6    -6.7\n",
       "2  12.0  59.0    -3.6   -20.7\n",
       "3  15.0  80.0    -0.6     0.3\n",
       "4  16.0  93.0     0.4    13.3\n",
       "5  14.0  85.0    -1.6     5.3\n",
       "6  16.0  66.0     0.4   -13.7\n",
       "7  16.0  79.0     0.4    -0.7\n",
       "8  18.0  77.0     2.4    -2.7\n",
       "9  19.0  91.0     3.4    11.3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "#x-xbar means the differnece between x and mean x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fde2dc-024c-48bc-9a76-9aaeb69b4a42",
   "metadata": {
    "id": "49fde2dc-024c-48bc-9a76-9aaeb69b4a42",
    "outputId": "594ae9a5-d993-41bf-d1ba-87a91a2870e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x-xbar</th>\n",
       "      <th>y-ybar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.6</td>\n",
       "      <td>-6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.6</td>\n",
       "      <td>-20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.6</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4</td>\n",
       "      <td>-13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.4</td>\n",
       "      <td>-2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.4</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x-xbar  y-ybar\n",
       "0     1.4    14.3\n",
       "1    -2.6    -6.7\n",
       "2    -3.6   -20.7\n",
       "3    -0.6     0.3\n",
       "4     0.4    13.3\n",
       "5    -1.6     5.3\n",
       "6     0.4   -13.7\n",
       "7     0.4    -0.7\n",
       "8     2.4    -2.7\n",
       "9     3.4    11.3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so let's calculate covariance\n",
    "#when x moves right of the mean, what does y do? oes it also move in the same direction\n",
    "df[['x-xbar', 'y-ybar']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fedc0-196e-4847-afc5-05f49466aa7a",
   "metadata": {
    "id": "5e6fedc0-196e-4847-afc5-05f49466aa7a"
   },
   "source": [
    "correlation\n",
    "$$\n",
    "{\\displaystyle r_{xy}={\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d119c6-d006-4001-9453-f313b7e77fc3",
   "metadata": {
    "id": "31d119c6-d006-4001-9453-f313b7e77fc3",
    "outputId": "e8be0229-895e-4b18-d7b4-342eeb399d81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960947613894623"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#idk is there a mistake in this wikipeadia formula - wheere is dividing by N ?\n",
    "#numerator\n",
    "num = (df['x-xbar']*df['y-ybar']).sum() #biased\n",
    "denom = (np.sqrt((df['x-xbar']**2).sum()))*(np.sqrt((df['y-ybar']**2).sum()))\n",
    "num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea329e7-a66c-4d03-ab5f-8e110a748092",
   "metadata": {
    "id": "6ea329e7-a66c-4d03-ab5f-8e110a748092",
    "outputId": "353ab44a-4481-4e15-8fa9-8a2ad4f74139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.59609476],\n",
       "       [0.59609476, 1.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df.x, df.y) #is this unbiased or biased\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eedc84-6d0d-4ff2-8a0f-193c2972a687",
   "metadata": {
    "id": "f5eedc84-6d0d-4ff2-8a0f-193c2972a687"
   },
   "outputs": [],
   "source": [
    "biased_sd_x = np.sqrt((df['x-xbar']**2).sum()/df.shape[0]) #biased - what we got with expectation\n",
    "unbiased_sd_x = np.sqrt((df['x-xbar']**2).sum()/(df.shape[0]-1)) #unbiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073262f2-0ca3-48ad-a0bd-f96fd95aafb7",
   "metadata": {
    "id": "073262f2-0ca3-48ad-a0bd-f96fd95aafb7"
   },
   "outputs": [],
   "source": [
    "biased_sd_y = np.sqrt((df['y-ybar']**2).sum()/df.shape[0]) #biased - what we got with eypectation\n",
    "unbiased_sd_y = np.sqrt((df['y-ybar']**2).sum()/(df.shape[0]-1)) #unbiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf64e8e-3af5-4fa8-80d7-059327a0130f",
   "metadata": {
    "id": "fdf64e8e-3af5-4fa8-80d7-059327a0130f"
   },
   "outputs": [],
   "source": [
    "biased_cov = (df['x-xbar']*df['y-ybar']).sum()/df.shape[0] #biased\n",
    "unbiased_cov = (df['x-xbar']*df['y-ybar']).sum()/(df.shape[0]-1) #biased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4dbf10-6940-4cd3-b238-41f27ed18e6b",
   "metadata": {
    "id": "9f4dbf10-6940-4cd3-b238-41f27ed18e6b",
    "outputId": "89054531-430b-4c52-ab9d-813fb2c5ca78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960947613894624"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biased_cov/(biased_sd_x*biased_sd_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe44a29-15a5-4d1a-912a-84b7a69bf5b6",
   "metadata": {
    "id": "dfe44a29-15a5-4d1a-912a-84b7a69bf5b6",
    "outputId": "e79062b7-1cbb-4a79-9e62-4b3bc60f4538"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960947613894623"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbiased_cov/(unbiased_sd_x*unbiased_sd_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ac06d-af33-43a8-b43d-2b2749ce2e86",
   "metadata": {
    "id": "1e1ac06d-af33-43a8-b43d-2b2749ce2e86",
    "outputId": "328823cf-6d7c-4480-96f6-52631155aaba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6623275126549582"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbiased_cov/(biased_sd_x*biased_sd_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9f5bc-07f2-430a-94ca-8b5e94966ce8",
   "metadata": {
    "id": "acd9f5bc-07f2-430a-94ca-8b5e94966ce8",
    "outputId": "c3e403f1-43cb-456d-a5ad-97134c314863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.79999999999998"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['x-xbar']*df['y-ybar']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade12910-f9ae-4992-9bda-d6bf65934388",
   "metadata": {
    "id": "ade12910-f9ae-4992-9bda-d6bf65934388"
   },
   "source": [
    "covariance - this is not covariance !\n",
    "\n",
    "$$\n",
    "\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})\n",
    "$$\n",
    "\n",
    "where is the 1/n or 1/n-1 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285b98e-636e-4f19-aafe-346a2fca9922",
   "metadata": {
    "id": "4285b98e-636e-4f19-aafe-346a2fca9922",
    "outputId": "7c4ca845-5f35-4e37-9ca6-ff9a3e5d7c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    20.02\n",
      "1    17.42\n",
      "2    74.52\n",
      "3    -0.18\n",
      "4     5.32\n",
      "5    -8.48\n",
      "6    -5.48\n",
      "7    -0.28\n",
      "8    -6.48\n",
      "9    38.42\n",
      "dtype: float64\n",
      "cov biased i.e /n or with expectations 13.479999999999999\n",
      "cov, unbiased i.e /n-1 14.977777777777776\n"
     ]
    }
   ],
   "source": [
    "#we can see that covariance gives dirrection to correlation\n",
    "#since the formula corr(x,y) = Cov(xy)/ (sd_x*sd_y)\n",
    "#and standard deviations are expected squared deviation from mean (or sum of squared deviation divided by the #observations (minus one if unbiased))\n",
    "\n",
    "#the numerator is covariance\n",
    "print(df['x-xbar']*df['y-ybar'])\n",
    "print('cov biased i.e /n or with expectations',(df['x-xbar']*df['y-ybar']).sum()/df.shape[0]) #biased\n",
    "print('cov, unbiased i.e /n-1',(df['x-xbar']*df['y-ybar']).sum()/(df.shape[0]-1)) #biased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee01c32-231c-45bc-9f46-7b2cdec2a4df",
   "metadata": {
    "id": "6ee01c32-231c-45bc-9f46-7b2cdec2a4df",
    "outputId": "4d55c56f-c6de-4053-ffd4-11305ada9000"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.71111111,  14.97777778],\n",
       "       [ 14.97777778, 134.01111111]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(df.x, df.y, ddof = 0) #biased\n",
    "np.cov(df.x, df.y, ddof = 1) #default = unbiases\n",
    "#but the numpy command covariance included the ubiased estimate - interesting !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516828eb-181d-4a5e-8d99-58cc97576d4d",
   "metadata": {
    "id": "516828eb-181d-4a5e-8d99-58cc97576d4d",
    "outputId": "ef3232ac-d159-48f1-a686-598914f2c29b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.24"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05c076-1b73-490e-865c-6ff0a86a8ba9",
   "metadata": {
    "id": "0e05c076-1b73-490e-865c-6ff0a86a8ba9",
    "outputId": "048ec965-c81d-4f79-b1d1-008154b4a2ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.711111111111111"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df.x, ddof = 1) #we specify that we want it ubiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26719da-f227-4e6f-a6f6-57d1cb36e228",
   "metadata": {
    "id": "b26719da-f227-4e6f-a6f6-57d1cb36e228"
   },
   "outputs": [],
   "source": [
    "## numpy variance from an array is biased - that is interesting\n",
    "## it is the same as we aclculated manually with expectaion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3e64a-382a-4137-a2f5-fbb6fa0a7634",
   "metadata": {
    "id": "38d3e64a-382a-4137-a2f5-fbb6fa0a7634",
    "outputId": "3aa51ea2-3366-4321-eac5-a96320250626"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.711111111111111"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.x.var() #while if we take it from a dataframe it is unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74066054-b82f-458d-9895-2b7252d9bb2c",
   "metadata": {
    "id": "74066054-b82f-458d-9895-2b7252d9bb2c",
    "outputId": "b321abd7-9094-46a3-c14e-2b5e58df4cb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x      4.711111\n",
       "y    134.011111\n",
       "Name: std, dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().iloc[2][:2]**2 #unbiased from describe command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee7548-9819-4f0a-a2e9-82c86ae235ca",
   "metadata": {
    "id": "efee7548-9819-4f0a-a2e9-82c86ae235ca",
    "outputId": "ce8514a9-c00e-4e25-dea4-76da46e21ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5960947613894589"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = df.shape[0]\n",
    "x_bar = df.x.mean()\n",
    "y_bar = df.y.mean()\n",
    "\n",
    "\n",
    "print('n' , n)\n",
    "((df.x*df.y).sum() - (n*x_bar*y_bar))/(np.sqrt((df.x**2).sum()- n*x_bar**2)*np.sqrt((df.y**2).sum()- n*y_bar**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a25f06-d79d-4a5b-82b3-77b861992a35",
   "metadata": {
    "id": "61a25f06-d79d-4a5b-82b3-77b861992a35",
    "outputId": "a8136115-a320-4a7c-8452-8c75f6fafa45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960947613894589"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "((df.x*df.y).sum() - n*x_bar*y_bar)/np.sqrt((df.x**2).sum() - n*(x_bar)**2)/np.sqrt((df.y**2).sum() - n*(y_bar)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95467cfa-61e7-4ace-8c53-fe876d9f60fa",
   "metadata": {
    "id": "95467cfa-61e7-4ace-8c53-fe876d9f60fa",
    "outputId": "efa9c3ca-dd53-4122-b40a-88e8742257b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5960947613894623\n",
      "0.5960947613894625\n",
      "0.5364852852505161\n",
      "0.6623275126549584\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "n = df.shape[0]-1 #unbiased\n",
    "print(((((df.x-x_bar)/unbiased_sd_x)*((df.y-y_bar)/unbiased_sd_y)).sum())/n)\n",
    "\n",
    "n = df.shape[0] #biased\n",
    "print(((((df.x-x_bar)/biased_sd_x)*((df.y-y_bar)/biased_sd_y)).sum())/n)\n",
    "\n",
    "#problems\n",
    "n = df.shape[0] #biased\n",
    "print(((((df.x-x_bar)/unbiased_sd_x)*((df.y-y_bar)/unbiased_sd_y)).sum())/n)\n",
    "\n",
    "#problems\n",
    "n = df.shape[0] -1 #unbiased\n",
    "print(((((df.x-x_bar)/biased_sd_x)*((df.y-y_bar)/biased_sd_y)).sum())/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57477ee6-60e3-4e54-b7e8-ae7deb744322",
   "metadata": {
    "id": "57477ee6-60e3-4e54-b7e8-ae7deb744322"
   },
   "outputs": [],
   "source": [
    "n = df.shape[0] #biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09426c4-2c49-4737-bce1-2d3f3ffa12f7",
   "metadata": {
    "id": "a09426c4-2c49-4737-bce1-2d3f3ffa12f7",
    "outputId": "1fbaf525-14a8-4216-9eff-e55d6ef8fe81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.479999999999999"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['x-xbar']*df['y-ybar']).sum()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c3436-c494-45d0-8c69-f39bf817ee65",
   "metadata": {
    "id": "7b3c3436-c494-45d0-8c69-f39bf817ee65",
    "outputId": "4fdb8ed4-f8aa-4de7-a08d-fce91cc0be21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.4\n",
       "1   -2.6\n",
       "2   -3.6\n",
       "3   -0.6\n",
       "4    0.4\n",
       "5   -1.6\n",
       "6    0.4\n",
       "7    0.4\n",
       "8    2.4\n",
       "9    3.4\n",
       "Name: x-xbar, dtype: float64"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['x-xbar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467d61d-17bd-4636-9329-63e0c3f7ac22",
   "metadata": {
    "id": "c467d61d-17bd-4636-9329-63e0c3f7ac22",
    "outputId": "5af55fb2-3097-4a26-fe0d-12e2c651c3fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.679900\n",
       "1   -1.262672\n",
       "2   -1.748315\n",
       "3   -0.291386\n",
       "4    0.194257\n",
       "5   -0.777029\n",
       "6    0.194257\n",
       "7    0.194257\n",
       "8    1.165543\n",
       "9    1.651186\n",
       "Name: x-xbar, dtype: float64"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['x-xbar']/biased_sd_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a76eb-df8f-40d8-aec7-b76661dbb7f3",
   "metadata": {
    "id": "c98a76eb-df8f-40d8-aec7-b76661dbb7f3",
    "outputId": "5f8df367-8de8-463f-d896-035b4ae27eb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14.3\n",
       "1    -6.7\n",
       "2   -20.7\n",
       "3     0.3\n",
       "4    13.3\n",
       "5     5.3\n",
       "6   -13.7\n",
       "7    -0.7\n",
       "8    -2.7\n",
       "9    11.3\n",
       "Name: y-ybar, dtype: float64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y-ybar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721786a5-2034-4670-9477-dc50d32a8bb7",
   "metadata": {
    "id": "721786a5-2034-4670-9477-dc50d32a8bb7",
    "outputId": "8d780d79-7eb4-4d08-ed4c-9900186445f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.302100\n",
       "1   -0.610075\n",
       "2   -1.884858\n",
       "3    0.027317\n",
       "4    1.211044\n",
       "5    0.482597\n",
       "6   -1.247467\n",
       "7   -0.063739\n",
       "8   -0.245851\n",
       "9    1.028932\n",
       "Name: y-ybar, dtype: float64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y-ybar']/biased_sd_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5206b-9e15-45d1-bf12-29b65e09b0e9",
   "metadata": {
    "id": "79e5206b-9e15-45d1-bf12-29b65e09b0e9",
    "outputId": "ea20417a-d65d-4783-a2df-75041c853497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960947613894625"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df['x-xbar']/biased_sd_x)*(df['y-ybar']/biased_sd_y)).mean() # unbiased correlation since biases (division by n) canel out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3d137-5d72-4c2e-87b9-a33e2d51dbde",
   "metadata": {
    "id": "b0b3d137-5d72-4c2e-87b9-a33e2d51dbde",
    "outputId": "7ab4dd19-cb39-4222-be45-00e8f6078576"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.479999999999999"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df['x-xbar']/1)*(df['y-ybar']/1)).mean() #biased covariance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
